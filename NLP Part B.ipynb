{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Part B: Spellchecker and Autocorrector Application\n",
    "____________________________\n",
    "Created by: Group 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data PreProcessing\n",
    "Data preprocessing stages are split into the following parts:\n",
    "+ #### Tokenization of Words\n",
    "+ #### Case Normalization\n",
    "+ #### Removing the following:\n",
    "  - Punctuation\n",
    "  - Stop Words\n",
    "  - Numeric Characters\n",
    "  - Special Characters\n",
    "  - Accented Characters\n",
    "\n",
    "+ #### Stemming and Lemmatization?\n",
    "  \n",
    "<u><i>More Text Cleaning Considerations:</i></u>\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings.\n",
    "- Resolve contractions for casual text.\n",
    "\n",
    "-----------------------------------------\n",
    "### 4. Design Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\tYour application must be able to find the spelling errors and suggest a few words to the user to modify the text.\n",
    "\n",
    "c)\tThe spelling errors that need to be addressed by your system are:\n",
    "\n",
    "i.\tNon-words (wrong spelling, where the word does not exist)\n",
    "\n",
    "ii.\tReal-words (wrong spelling due to wrong context, but the misspelt word does exist)\n",
    "    - Grammatical errors, typos e.t.c\n",
    "\n",
    "d)\tThe techniques used for the detection of the spelling errors must include: <body>\n",
    "  <p style=\"color:rgb(255,0,0);\"> - Bigrams</p>\n",
    "     <p style=\"color:rgb(255,0,0);\"> - Minimum Edit Distance,</p>\n",
    "     <p style=\"color:rgb(255,0,0);\">- Other suitable popular techniques used in NLP</p>\n",
    "   </body>\n",
    "\n",
    "<p>e)\tProvide the following functionality in your application: </p>\n",
    "<p>   Ability to show a sorted list of all words in the corpus with the facility of exploring the list and search for a     specific word.</p>\n",
    "\n",
    "\tAbility to highlight the misspelled words, and right click to suggest the correct words (with their minimum edit     distance from the wrong word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Corpus and Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "# import packages\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "import unicodedata\n",
    "import heapq                               \n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 100 tradÃ©Â®â€\\xa0Â¥mark! â„¢ Â® Reading Books The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\nTranslated by David Wyllie.\\n\\nThis eBook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\nre-use it under the terms of the Project Gutenberg License included\\nwith this eBook or online at www.gutenberg.net\\n\\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\\n**     Please follow the copyright guidelines in this file.     **\\n\\nÃŸ\\nTitle: Metamorphosis\\n\\nAuthor: Franz Kafka\\n\\nTranslator: David Wyllie\\n\\nRelease Date: August 16, 2005 [EBook #5200]\\nFirst posted: May 13, 2002\\nLast updated: May 20, 2012\\n\\nLanguage: English\\n\\n\\n*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\n\\n\\n\\n\\nCopyright (C) 2002 David Wyllie.\\n\\n\\n\\n\\n\\n  Metamorphosis\\n  Franz Kafka\\n\\nTranslated by David Wyllie\\n\\n\\n\\nI\\n\\n\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', '100', 'tradÃ©Â®â€', 'Â¥mark!', 'â„¢', 'Â®', 'Reading', 'Books', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis,', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie.', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.net', '**', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook,', 'Details', 'Below', '**', '**', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file.', '**', 'ÃŸ', 'Title:', 'Metamorphosis', 'Author:', 'Franz', 'Kafka', 'Translator:', 'David', 'Wyllie', 'Release', 'Date:', 'August', '16,', '2005']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "\n",
    "# split based on words only\n",
    "\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '100', 'tradã©â®â€', 'â¥mark!', 'â„¢', 'â®', 'reading', 'books', 'the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis,', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie.', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'you', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.net', '**', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook,', 'details', 'below', '**', '**', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file.', '**', 'ãÿ', 'title:', 'metamorphosis', 'author:', 'franz', 'kafka', 'translator:', 'david', 'wyllie', 'release', 'date:', 'august', '16,', '2005']\n"
     ]
    }
   ],
   "source": [
    "# text normalization - convert to lower case\n",
    "nor_words = [word.lower() for word in words]\n",
    "print(nor_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped_words = [w.translate(table) for w in nor_words]\n",
    "print(stripped_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', 'tradã©â®â€', 'â¥mark', 'â„¢', 'â®', 'reading', 'books', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'reuse', 'terms', 'project', 'gutenberg', 'license', 'included', 'ebook', 'online', 'wwwgutenbergnet', '', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', '', '', 'please', 'follow', 'copyright', 'guidelines', 'file', '', 'ãÿ', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', '16', '2005', 'ebook', '5200', 'first', 'posted', 'may', '13', '2002', 'last', 'updated', 'may', '20', '2012', 'language', 'english', '', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosis', '', 'copyright', 'c', '2002', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "no_st_words = [w for w in stripped_words if not w in stop_words]\n",
    "print(no_st_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Numeric Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tradã©â®â€ â¥mark â„¢ â® reading books project gutenberg ebook metamorphosis franz kafka translated \n"
     ]
    }
   ],
   "source": [
    "#Remove numeric characters\n",
    "no_numbers = ' '.join(c for c in no_st_words if not c.isdigit())\n",
    "print(no_numbers[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tradãââ âmark â â reading books project gutenberg ebook metamorphosis franz kafka translated david w\n"
     ]
    }
   ],
   "source": [
    "# function to remove special characters\n",
    "def remove_s_c(no_numbers):\n",
    "    # define the pattern to keep\n",
    "    rem = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\w+)]' \n",
    "    return re.sub(rem, '',no_numbers)\n",
    " \n",
    "# calling the function\n",
    "no_sc_words = remove_s_c(no_numbers)\n",
    "\n",
    "print(no_sc_words[:100])\n",
    "\n",
    "# resulting in double spaces after removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tradaaa amark a a reading books project gutenberg ebook metamorphosis franz kafka translated david w\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# function to remove accented characters\n",
    "def remove_a_c(no_sc_words):\n",
    "    new_text = unicodedata.normalize('NFKD', no_sc_words).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "# call function\n",
    "no_ac_words = remove_a_c(no_sc_words)\n",
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ac_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tradaaa amark a a reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie ebook use anyone anywhere cost almost restrictions whatsoever may copy give away reuse terms project gutenberg license included ebook online wwwgutenbergnet copyrighted project gutenberg ebook details please follow copyright guidelines file ay title metamorphosis author franz kafka translator david wyllie release date august ebook first posted may last updated may language english start project gutenberg ebook metamorphosis copyright c david wyllie metamorphosis franz kafka translated david wyllie one morning gregor samsa woke troubled dreams found transformed bed horrible vermin lay armourlike back lifted head little could see brown belly slightly domed divided arches stiff sections bedding hardly able cover seemed ready slide moment many legs pitifully thin compared size rest waved helplessly looked whats happened thought wasnt dream room proper human room although little small l\n"
     ]
    }
   ],
   "source": [
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ws_words[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(no_ws_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-700a19d2b0db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_ws_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-700a19d2b0db>\u001b[0m in \u001b[0;36mConvert\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_ws_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "def Convert(string): \n",
    "    li = list(string.split(\" \")) \n",
    "    return li \n",
    "print(Convert(no_ws_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-62366a2abd97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_ws_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-700a19d2b0db>\u001b[0m in \u001b[0;36mConvert\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_ws_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "a = Convert(no_ws_words)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do List\n",
    "\n",
    "#1. Expand Contractions\n",
    "#2. Create Dictionary\n",
    "#3. Add Unigrams / Bigrams\n",
    "#4. Check repeatition / unnecessary imported libraries\n",
    "#5. Create Copy of File After Every Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d\\w\\d|\\s\\d+$\", \" \",text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text \n",
    "clean_data = []\n",
    "\n",
    "for data in list:\n",
    "    clean_data.append(clean_text(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tradaaa',\n",
       " 'amark',\n",
       " 'a',\n",
       " 'a',\n",
       " 'reading',\n",
       " 'books',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'metamorphosis',\n",
       " 'franz',\n",
       " 'kafka',\n",
       " 'translated',\n",
       " 'david',\n",
       " 'wyllie',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'reuse',\n",
       " 'terms',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'license',\n",
       " 'included',\n",
       " 'ebook',\n",
       " 'online',\n",
       " 'wwwgutenbergnet',\n",
       " 'copyrighted',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'details',\n",
       " 'please',\n",
       " 'follow',\n",
       " 'copyright',\n",
       " 'guidelines',\n",
       " 'file',\n",
       " 'ay',\n",
       " 'title',\n",
       " 'metamorphosis',\n",
       " 'author']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
