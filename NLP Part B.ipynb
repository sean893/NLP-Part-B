{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Part B: Spellchecker and Autocorrector Application\n",
    "____________________________\n",
    "Created by: Group 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "##### https://docs.python.org/2/howto/regex.html\n",
    "##### https://www.languagetool.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data PreProcessing\n",
    "Data preprocessing stages are split into the following parts:\n",
    "+ #### Tokenization of Words\n",
    "+ #### Case Normalization\n",
    "+ #### Removing the following:\n",
    "  - Punctuation\n",
    "  - Stop Words\n",
    "  - Numeric Characters\n",
    "  - Special Characters\n",
    "  - Accented Characters\n",
    "\n",
    "+ #### Stemming and Lemmatization?\n",
    "  \n",
    "<u><i>More Text Cleaning Considerations:</i></u>\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings.\n",
    "- Resolve contractions for casual text.\n",
    "- References: shorturl.at/pvHS8\n",
    "\n",
    "-----------------------------------------\n",
    "### 4. Design Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\tYour application must be able to find the spelling errors and suggest a few words to the user to modify the text.\n",
    "\n",
    "c)\tThe spelling errors that need to be addressed by your system are:\n",
    "\n",
    "i.\tNon-words (wrong spelling, where the word does not exist)\n",
    "\n",
    "ii.\tReal-words (wrong spelling due to wrong context, but the misspelt word does exist)\n",
    "    - Grammatical errors, typos e.t.c\n",
    "\n",
    "d)\tThe techniques used for the detection of the spelling errors must include: <body>\n",
    "  <p style=\"color:rgb(255,0,0);\"> - Bigrams</p>\n",
    "     <p style=\"color:rgb(255,0,0);\"> - Minimum Edit Distance,</p>\n",
    "     <p style=\"color:rgb(255,0,0);\">- Other suitable popular techniques used in NLP</p>\n",
    "   </body>\n",
    "\n",
    "<p>e)\tProvide the following functionality in your application: </p>\n",
    "<p>   Ability to show a sorted list of all words in the corpus with the facility of exploring the list and search for a     specific word.</p>\n",
    "\n",
    "\tAbility to highlight the misspelled words, and right click to suggest the correct words (with their minimum edit     distance from the wrong word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preriquisites Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# import packages\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import heapq                               \n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f30b30eeaa7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# split into words by white space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# split based on words only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "\n",
    "# split based on words only\n",
    "\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text normalization - convert to lower case\n",
    "nor_words = [word.lower() for word in words]\n",
    "#print(nor_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped_words = [w.translate(table) for w in nor_words]\n",
    "#print(stripped_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "no_st_words = [w for w in stripped_words if not w in stop_words]\n",
    "#print(no_st_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Numeric Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove numeric characters\n",
    "no_numbers = ' '.join(c for c in no_st_words if not c.isdigit())\n",
    "#print(no_numbers[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tradémark   reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie \n"
     ]
    }
   ],
   "source": [
    "# function to remove special characters\n",
    "def remove_s_c(no_numbers):\n",
    "    # define the pattern to keep\n",
    "    rem = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\w+)]' \n",
    "    return re.sub(rem, '',no_numbers)\n",
    " \n",
    "# calling the function\n",
    "no_sc_words = remove_s_c(no_numbers)\n",
    "\n",
    "print(no_sc_words[:100])\n",
    "\n",
    "# resulting in double spaces after removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trademark   reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie \n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# function to remove accented characters\n",
    "def remove_a_c(no_sc_words):\n",
    "    new_text = unicodedata.normalize('NFKD', no_sc_words).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "# call function\n",
    "no_ac_words = remove_a_c(no_sc_words)\n",
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ac_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trademark reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie eb\n"
     ]
    }
   ],
   "source": [
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ws_words[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycontractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-59d8ec24534a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycontractions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you're happy now\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# \"you are happy now\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yall're happy now\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# default: true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycontractions'"
     ]
    }
   ],
   "source": [
    "from pycontractions import Contractions\n",
    "\n",
    "contractions.fix(\"you're happy now\")\n",
    "# \"you are happy now\"\n",
    "contractions.fix(\"yall're happy now\", slang=False) # default: true\n",
    "# \"yall are happy\"\n",
    "contractions.fix(\"yall're happy now\")\n",
    "# \"you all are happy now\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-d7eab82b0c99>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-d7eab82b0c99>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install contractions\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install contractions\n",
    "#conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pathlib\n",
    "# write to file\n",
    "#pathlib.Path(\"output.txt\").write_text((no_sc_words))\n",
    "\n",
    "#import re\n",
    "\n",
    "#fin = open(\"data.txt\", \"rt\")\n",
    "#fout = open(\"out.txt\", \"wt\")\n",
    "\n",
    "#for line in fin:\n",
    "#\tfout.write(re.sub('\\s+',' ',line))\n",
    "#\t\n",
    "#fin.close()\n",
    "#fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the SnowballStemmer which is based on The Porter Stemming Algorithm\n",
    "##snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "##word_tokens = nltk.word_tokenize(no_ac_words)\n",
    "##stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
    "#print(stemmed_word[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "##wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "##word_tokens = nltk.word_tokenize(no_ac_words)\n",
    "##lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "#print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Design Deliverables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
