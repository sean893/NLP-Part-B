{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Part B: Spellchecker and Autocorrector Application\n",
    "____________________________\n",
    "Created by: Group 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data PreProcessing\n",
    "Data preprocessing stages are split into the following parts:\n",
    "+ #### Tokenization of Words\n",
    "+ #### Case Normalization\n",
    "+ #### Removing the following:\n",
    "  - Punctuation\n",
    "  - Stop Words\n",
    "  - Numeric Characters\n",
    "  - Special Characters\n",
    "  - Accented Characters\n",
    "\n",
    "+ #### Stemming and Lemmatization?\n",
    "  \n",
    "<u><i>More Text Cleaning Considerations:</i></u>\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings.\n",
    "- Resolve contractions for casual text.\n",
    "\n",
    "-----------------------------------------\n",
    "### 4. Design Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\tYour application must be able to find the spelling errors and suggest a few words to the user to modify the text.\n",
    "\n",
    "c)\tThe spelling errors that need to be addressed by your system are:\n",
    "\n",
    "i.\tNon-words (wrong spelling, where the word does not exist)\n",
    "\n",
    "ii.\tReal-words (wrong spelling due to wrong context, but the misspelt word does exist)\n",
    "    - Grammatical errors, typos e.t.c\n",
    "\n",
    "d)\tThe techniques used for the detection of the spelling errors must include: <body>\n",
    "  <p style=\"color:rgb(255,0,0);\"> - Bigrams</p>\n",
    "     <p style=\"color:rgb(255,0,0);\"> - Minimum Edit Distance,</p>\n",
    "     <p style=\"color:rgb(255,0,0);\">- Other suitable popular techniques used in NLP</p>\n",
    "   </body>\n",
    "\n",
    "<p>e)\tProvide the following functionality in your application: </p>\n",
    "<p>   Ability to show a sorted list of all words in the corpus with the facility of exploring the list and search for a     specific word.</p>\n",
    "\n",
    "\tAbility to highlight the misspelled words, and right click to suggest the correct words (with their minimum edit     distance from the wrong word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Corpus and Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "\n",
    "# import packages\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "import unicodedata\n",
    "import heapq                               \n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "\n",
    "# split based on words only\n",
    "\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "#print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text normalization - convert to lower case\n",
    "nor_words = [word.lower() for word in words]\n",
    "#print(nor_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped_words = [w.translate(table) for w in nor_words]\n",
    "#print(stripped_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "no_st_words = [w for w in stripped_words if not w in stop_words]\n",
    "#print(no_st_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Numeric Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove numeric characters\n",
    "no_numbers = ' '.join(c for c in no_st_words if not c.isdigit())\n",
    "#print(no_numbers[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tradémark   reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie \n"
     ]
    }
   ],
   "source": [
    "# function to remove special characters\n",
    "def remove_s_c(no_numbers):\n",
    "    # define the pattern to keep\n",
    "    rem = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\w+)]' \n",
    "    return re.sub(rem, '',no_numbers)\n",
    " \n",
    "# calling the function\n",
    "no_sc_words = remove_s_c(no_numbers)\n",
    "\n",
    "print(no_sc_words[:100])\n",
    "\n",
    "# resulting in double spaces after removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trademark   reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie \n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# function to remove accented characters\n",
    "def remove_a_c(no_sc_words):\n",
    "    new_text = unicodedata.normalize('NFKD', no_sc_words).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "# call function\n",
    "no_ac_words = remove_a_c(no_sc_words)\n",
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ac_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trademark reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie eb\n"
     ]
    }
   ],
   "source": [
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ws_words[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do List\n",
    "\n",
    "#1. Expand Contractions\n",
    "#2. Create Dictionary\n",
    "#3. Add Unigrams / Bigrams\n",
    "#4. Check repeatition / unnecessary imported libraries\n",
    "#5. Create Copy of File After Every Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"\\n\", \"\",  text)\n",
    "    text = re.sub(r\"[-()]\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \" .\", text)\n",
    "    text = re.sub(r\"\\!\", \" !\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r\"\\,\", \" ,\", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d\\w\\d|\\s\\d+$\", \" \",text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"we'd\", \"we would\", text)\n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text \n",
    "clean_data = []\n",
    "\n",
    "for data in text:\n",
    "    clean_data.append(clean_text(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
