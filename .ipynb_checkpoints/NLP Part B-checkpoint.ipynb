{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Part B: Spellchecker and Autocorrector Application\n",
    "____________________________\n",
    "Created by: Group 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries for this task.\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(sent):  \n",
    "    \"\"\"Parsing the sentence to corrected and original and storing in the dictionary.\"\"\"\n",
    "    loriginal = []\n",
    "    lcorrected = []\n",
    "    lcorr = []\n",
    "    indexes = []\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in sent:\n",
    "        if '|' in i:\n",
    "            # Splitting the sentence on '|'\n",
    "            str1 = i.split('|')\n",
    "            # Previous word to '|' is storing in loriginal list.\n",
    "            loriginal.append(str1[0])\n",
    "            # Next word to '|' is storing in lcorrected list.\n",
    "            lcorrected.append(str1[1])\n",
    "            #Noting down the index of error.\n",
    "            indexes.append(cnt)\n",
    "        \n",
    "        else:\n",
    "            # If there is no '|' in sentence, sentence is stored in loriginal and lcorrected as it is.\n",
    "            loriginal.append(i)\n",
    "            lcorrected.append(i)\n",
    "        cnt = cnt+1\n",
    "        \n",
    "    #Loading to loriginal, lcorrected and index list to dictionary.      \n",
    "    dictionary = {'original': loriginal, 'corrected': lcorrected, 'indexes': indexes}\n",
    "    \n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d96fc0599f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocessing():\n",
    "    \"\"\"Loading the data from 'holbrook.txt' and passing to parsing function to get parssed sentences. \n",
    "    Returning the whole dictionary as data.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Reading the txt file\n",
    "    text_file = open(\"metamorphosis_cleant\", \"r\")\n",
    "    lines = []\n",
    "    for i in text_file:\n",
    "        lines.append(i.strip())\n",
    "    \n",
    "    # Word tokenizing the sentences\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in lines]\n",
    "    \n",
    "    # Calling a parse function to get corrected, original sentences.\n",
    "    for sent in sentences:\n",
    "        data.append(parsing(sent))\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "##### https://www.languagetool.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data PreProcessing\n",
    "Data preprocessing stages are split into the following parts:\n",
    "+ #### Tokenization of Words\n",
    "+ #### Case Normalization\n",
    "+ #### Removing the following:\n",
    "  - Punctuation\n",
    "  - Stop Words\n",
    "  - Numeric Characters\n",
    "  - Special Characters\n",
    "  - Accented Characters\n",
    "\n",
    "+ #### Stemming and Lemmatization?\n",
    "  \n",
    "<u><i>More Text Cleaning Considerations:</i></u>\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings.\n",
    "- Resolve contractions for casual text.\n",
    "- References: shorturl.at/pvHS8\n",
    "\n",
    "-----------------------------------------\n",
    "### 4. Design Deliverables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\tYour application must be able to find the spelling errors and suggest a few words to the user to modify the text.\n",
    "\n",
    "c)\tThe spelling errors that need to be addressed by your system are:\n",
    "\n",
    "i.\tNon-words (wrong spelling, where the word does not exist)\n",
    "\n",
    "ii.\tReal-words (wrong spelling due to wrong context, but the misspelt word does exist)\n",
    "    - Grammatical errors, typos e.t.c\n",
    "\n",
    "d)\tThe techniques used for the detection of the spelling errors must include: <body>\n",
    "  <p style=\"color:rgb(255,0,0);\"> - Bigrams</p>\n",
    "     <p style=\"color:rgb(255,0,0);\"> - Minimum Edit Distance,</p>\n",
    "     <p style=\"color:rgb(255,0,0);\">- Other suitable popular techniques used in NLP</p>\n",
    "   </body>\n",
    "\n",
    "<p>e)\tProvide the following functionality in your application: </p>\n",
    "<p>   Ability to show a sorted list of all words in the corpus with the facility of exploring the list and search for a     specific word.</p>\n",
    "\n",
    "\tAbility to highlight the misspelled words, and right click to suggest the correct words (with their minimum edit     distance from the wrong word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preriquisites Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "#Add corpus to your working directory\n",
    "\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# import packages\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import heapq                               \n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', '100', 'tradé®†¥mark!', '™', '®', 'Reading', 'Books', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis,', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie.', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.net', '**', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook,', 'Details', 'Below', '**', '**', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file.', '**', 'ß', 'Title:', 'Metamorphosis', 'Author:', 'Franz', 'Kafka', 'Translator:', 'David', 'Wyllie', 'Release', 'Date:', 'August', '16,', '2005', '[EBook', '#5200]', 'First', 'posted:', 'May', '13,', '2002', 'Last', 'updated:', 'May', '20,', '2012', 'Language:', 'English', '***', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', '***', 'Copyright', '(C)', '2002', 'David', 'Wyllie.', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'I', 'One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small,', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'walls.', 'A', 'collection', 'of', 'textile', 'samples', 'lay', 'spread', 'out', 'on', 'the', 'table', '-', 'Samsa', 'was', 'a', 'travelling', 'salesman', '-', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice,', 'gilded', 'frame.', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright,', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer.', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather.', 'Drops', 'of', 'rain', 'could', 'be', 'heard', 'hitting', 'the', 'pane,', 'which', 'made', 'him', 'feel', 'quite', 'sad.', '\"How', 'about', 'if', 'I', 'sleep', 'a', 'little', 'bit', 'longer', 'and', 'forget', 'all', 'this', 'nonsense\",', 'he', 'thought,', 'but', 'that', 'was', 'something', 'he', 'was', 'unable', 'to', 'do', 'because', 'he', 'was', 'used', 'to', 'sleeping', 'on', 'his', 'right,', 'and', 'in', 'his', 'present', 'state', \"couldn't\", 'get', 'into', 'that', 'position.', 'However', 'hard', 'he', 'threw', 'himself', 'onto', 'his', 'right,', 'he', 'always', 'rolled', 'back', 'to', 'where', 'he', 'was.', 'He', 'must', 'have', 'tried', 'it', 'a', 'hundred', 'times,', 'shut', 'his', 'eyes', 'so', 'that', 'he', \"wouldn't\", 'have', 'to', 'look', 'at', 'the', 'floundering', 'legs,', 'and', 'only', 'stopped', 'when', 'he', 'began', 'to', 'feel', 'a', 'mild,', 'dull', 'pain', 'there', 'that', 'he', 'had', 'never', 'felt', 'before.', '\"Oh,', 'God\",', 'he', 'thought,', '\"what', 'a', 'strenuous', 'career', 'it', 'is', 'that', \"I've\", 'chosen!', 'Travelling', 'day', 'in', 'and', 'day', 'out.', 'Doing', 'business', 'like', 'this', 'takes', 'much', 'more', 'effort', 'than', 'doing', 'your', 'own', 'business', 'at', 'home,', 'and', 'on', 'top', 'of', 'that', \"there's\", 'the', 'curse', 'of', 'travelling,', 'worries', 'about', 'making', 'train', 'connections,', 'bad', 'and', 'irregular', 'food,', 'contact', 'with', 'different', 'people', 'all', 'the', 'time', 'so', 'that', 'you', 'can', 'never', 'get', 'to', 'know', 'anyone', 'or', 'become', 'friendly', 'with', 'them.', 'It', 'can', 'all', 'go', 'to', 'Hell!\"', 'He', 'felt', 'a', 'slight', 'itch', 'up', 'on', 'his', 'belly;', 'pushed', 'himself', 'slowly', 'up', 'on', 'his', 'back', 'towards', 'the', 'headboard', 'so', 'that', 'he', 'could', 'lift', 'his', 'head', 'better;', 'found', 'where', 'the', 'itch', 'was,', 'and', 'saw', 'that', 'it', 'was', 'covered', 'with', 'lots', 'of', 'little', 'white', 'spots', 'which', 'he', \"didn't\", 'know', 'what', 'to', 'make', 'of;', 'and', 'when', 'he', 'tried', 'to', 'feel', 'the', 'place', 'with', 'one', 'of', 'his', 'legs', 'he', 'drew', 'it', 'quickly', 'back', 'because', 'as', 'soon', 'as', 'he', 'touched', 'it', 'he', 'was', 'overcome', 'by', 'a', 'cold', 'shudder.', 'He', 'slid', 'back', 'into', 'his', 'former', 'position.', '\"Getting', 'up', 'early', 'all', 'the', 'time\",', 'he', 'thought,', '\"it', 'makes', 'you', 'stupid.', \"You've\", 'got', 'to', 'get', 'enough', 'sleep.', 'Other', 'travelling', 'salesmen', 'live', 'a', 'life', 'of', 'luxury.', 'For', 'instance,', 'whenever', 'I', 'go', 'back', 'to', 'the', 'guest', 'house', 'during', 'the', 'morning', 'to', 'copy', 'out', 'the', 'contract,', 'these', 'gentlemen', 'are', 'always', 'still', 'sitting', 'there', 'eating', 'their', 'breakfasts.', 'I', 'ought', 'to', 'just', 'try', 'that', 'with', 'my', 'boss;', \"I'd\", 'get', 'kicked', 'out', 'on', 'the', 'spot.', 'But', 'who', 'knows,', 'maybe', 'that', 'would', 'be', 'the', 'best', 'thing', 'for', 'me.', 'If', 'I', \"didn't\", 'have', 'my', 'parents', 'to', 'think', 'about', \"I'd\", 'have', 'given', 'in', 'my', 'notice', 'a', 'long', 'time', 'ago,', \"I'd\", 'have', 'gone', 'up', 'to', 'the', 'boss', 'and', 'told', 'him', 'just', 'what', 'I', 'think,', 'tell', 'him', 'everything', 'I', 'would,', 'let', 'him', 'know', 'just', 'what', 'I', 'feel.', \"He'd\", 'fall', 'right', 'off', 'his', 'desk!', 'And', \"it's\", 'a', 'funny', 'sort', 'of', 'business', 'to', 'be', 'sitting', 'up', 'there', 'at', 'your', 'desk,', 'talking', 'down', 'at', 'your', 'subordinates', 'from', 'up', 'there,', 'especially', 'when', 'you', 'have', 'to', 'go', 'right', 'up', 'close', 'because', 'the', 'boss', 'is', 'hard', 'of', 'hearing.', 'Well,', \"there's\", 'still', 'some', 'hope;', 'once', \"I've\", 'got', 'the', 'money', 'together', 'to', 'pay', 'off', 'my', \"parents'\", 'debt', 'to', 'him', '-', 'another', 'five', 'or', 'six', 'years', 'I', 'suppose', '-', \"that's\", 'definitely', 'what', \"I'll\", 'do.', \"That's\", 'when', \"I'll\", 'make', 'the', 'big', 'change.', 'First', 'of', 'all', 'though,', \"I've\", 'got', 'to', 'get', 'up,', 'my', 'train', 'leaves', 'at', 'five.\"', 'And', 'he', 'looked', 'over', 'at', 'the', 'alarm', 'clock,', 'ticking', 'on', 'the', 'chest', 'of', 'drawers.', '\"God', 'in', 'Heaven!\"', 'he', 'thought.', 'It', 'was', 'half', 'past', 'six', 'and', 'the', 'hands', 'were', 'quietly', 'moving', 'forwards,', 'it', 'was', 'even', 'later', 'than', 'half', 'past,', 'more', 'like', 'quarter', 'to', 'seven.', 'Had', 'the', 'alarm', 'clock', 'not', 'rung?', 'He', 'could', 'see', 'from', 'the', 'bed', 'that', 'it', 'had', 'been', 'set', 'for', 'four', \"o'clock\", 'as', 'it', 'should', 'have', 'been;', 'it', 'certainly', 'must', 'have', 'rung.', 'Yes,', 'but', 'was', 'it', 'possible', 'to', 'quietly', 'sleep', 'through', 'that', 'furniture-rattling', 'noise?', 'True,', 'he', 'had', 'not', 'slept', 'peacefully,', 'but', 'probably', 'all', 'the', 'more', 'deeply', 'because', 'of', 'that.', 'What', 'should', 'he', 'do', 'now?', 'The', 'next', 'train', 'went', 'at', 'seven;', 'if', 'he', 'were', 'to', 'catch', 'that', 'he', 'would', 'have', 'to', 'rush', 'like', 'mad', 'and', 'the', 'collection', 'of', 'samples', 'was', 'still', 'not', 'packed,', 'and', 'he', 'did', 'not', 'at', 'all', 'feel', 'particularly', 'fresh', 'and', 'lively.', 'And', 'even', 'if', 'he', 'did', 'catch', 'the', 'train', 'he']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "\n",
    "# split based on words only\n",
    "\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text normalization - convert to lower case\n",
    "nor_words = [word.lower() for word in words]\n",
    "#print(nor_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped_words = [w.translate(table) for w in nor_words]\n",
    "#print(stripped_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "no_st_words = [w for w in stripped_words if not w in stop_words]\n",
    "#print(no_st_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Numeric Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove numeric characters\n",
    "no_numbers = ' '.join(c for c in no_st_words if not c.isdigit())\n",
    "#print(no_numbers[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tradémark   reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie \n"
     ]
    }
   ],
   "source": [
    "# function to remove special characters\n",
    "def remove_s_c(no_numbers):\n",
    "    # define the pattern to keep\n",
    "    rem = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s\\w+)]' \n",
    "    return re.sub(rem, '',no_numbers)\n",
    " \n",
    "# calling the function\n",
    "no_sc_words = remove_s_c(no_numbers)\n",
    "\n",
    "print(no_sc_words[:100])\n",
    "\n",
    "# resulting in double spaces after removing special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trademark   reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie \n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# function to remove accented characters\n",
    "def remove_a_c(no_sc_words):\n",
    "    new_text = unicodedata.normalize('NFKD', no_sc_words).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "# call function\n",
    "no_ac_words = remove_a_c(no_sc_words)\n",
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ac_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trademark reading books project gutenberg ebook metamorphosis franz kafka translated david wyllie eb\n"
     ]
    }
   ],
   "source": [
    "no_ws_words = (\" \".join(no_ac_words.split()))\n",
    "\n",
    "print(no_ws_words[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycontractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-59d8ec24534a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycontractions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you're happy now\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# \"you are happy now\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yall're happy now\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# default: true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycontractions'"
     ]
    }
   ],
   "source": [
    "from pycontractions import Contractions\n",
    "\n",
    "contractions.fix(\"you're happy now\")\n",
    "# \"you are happy now\"\n",
    "contractions.fix(\"yall're happy now\", slang=False) # default: true\n",
    "# \"yall are happy\"\n",
    "contractions.fix(\"yall're happy now\")\n",
    "# \"you all are happy now\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-d7eab82b0c99>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-d7eab82b0c99>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install contractions\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install contractions\n",
    "#conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pathlib\n",
    "# write to file\n",
    "#pathlib.Path(\"output.txt\").write_text((no_sc_words))\n",
    "\n",
    "#import re\n",
    "\n",
    "#fin = open(\"data.txt\", \"rt\")\n",
    "#fout = open(\"out.txt\", \"wt\")\n",
    "\n",
    "#for line in fin:\n",
    "#\tfout.write(re.sub('\\s+',' ',line))\n",
    "#\t\n",
    "#fin.close()\n",
    "#fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the SnowballStemmer which is based on The Porter Stemming Algorithm\n",
    "##snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "##word_tokens = nltk.word_tokenize(no_ac_words)\n",
    "##stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
    "#print(stemmed_word[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "##wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "##word_tokens = nltk.word_tokenize(no_ac_words)\n",
    "##lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "#print(lemmatized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Design Deliverables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2bb1eb1b41f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcontractions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexpand_contractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontractions_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from contractions import contractions_dict\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "return expanded_text\n",
    "def main():\n",
    "    text = \"\"\"I ain't going there. You'll have to go alone.\"\"\"\n",
    "    \n",
    "    text=expand_contractions(text,contractions_dict)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    print (tokenized_sentences)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
